{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitvenvvenvb8d9021ca51d43cfbc34908d99b12918",
   "display_name": "Python 3.8.3 64-bit ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "breast_cancer = pd.read_csv(\"datasets/breast_cancer_diagnostic/data.csv\")\n",
    "breast_cancer.loc[breast_cancer['diagnosis'] == 'M', 'diagnosis'] = 1\n",
    "breast_cancer.loc[breast_cancer['diagnosis'] == 'B', 'diagnosis'] = 0\n",
    "X = breast_cancer.iloc[:, 2:-1].to_numpy()\n",
    "y = breast_cancer['diagnosis'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_min, X_max = np.min(X, axis=0), np.max(X, axis=0)\n",
    "X = (X - X_min) / (X_max - X_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logistic_regression import LogisticRegression\n",
    "from svm import SVM, SoftMarginSVM\n",
    "\n",
    "mix_ids = np.random.permutation(X.shape[0])\n",
    "train_N = int(X.shape[0] * .8)\n",
    "train_set, test_set = mix_ids[:train_N], mix_ids[train_N:]\n",
    "X_train, y_train = X[train_set], y[train_set]\n",
    "X_test, y_test = X[test_set], y[test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 1\tLoss: 0.42763710837531815\nEpoch: 2\tLoss: 0.33663383039254874\nEpoch: 3\tLoss: 0.3083370381360738\nEpoch: 4\tLoss: 0.2909927462971424\nEpoch: 5\tLoss: 0.32533802942553647\nEpoch: 6\tLoss: 0.2590101540891268\nEpoch: 7\tLoss: 0.24581469785960108\nEpoch: 8\tLoss: 0.2686124989816597\nEpoch: 9\tLoss: 0.23179159756522363\nEpoch: 10\tLoss: 0.22019061784230903\nEpoch: 11\tLoss: 0.2289416215566906\nEpoch: 12\tLoss: 0.22813245440048127\nEpoch: 13\tLoss: 0.21020134295915327\nEpoch: 14\tLoss: 0.19910502940165956\nEpoch: 15\tLoss: 0.22852967177899264\nEpoch: 16\tLoss: 0.1949716695302225\nEpoch: 17\tLoss: 0.18784774519201505\nEpoch: 18\tLoss: 0.19977806434985998\nEpoch: 19\tLoss: 0.18853118339060648\nEpoch: 20\tLoss: 0.18071960238029788\nEpoch: 21\tLoss: 0.17674444832392427\nEpoch: 22\tLoss: 0.17586092019972346\nEpoch: 23\tLoss: 0.18506100019829247\nEpoch: 24\tLoss: 0.18315671777891446\nEpoch: 25\tLoss: 0.16829605775680156\nEpoch: 26\tLoss: 0.16790119647940255\nEpoch: 27\tLoss: 0.16468380641303154\nEpoch: 28\tLoss: 0.16494619863168017\nEpoch: 29\tLoss: 0.16176650460172515\nEpoch: 30\tLoss: 0.16108113797262555\nEpoch: 31\tLoss: 0.1603947331043373\nEpoch: 32\tLoss: 0.15764255914644848\nEpoch: 33\tLoss: 0.15774950061979476\nEpoch: 34\tLoss: 0.17424688506324207\nEpoch: 35\tLoss: 0.15456078950838611\nEpoch: 36\tLoss: 0.15864222552386834\nEpoch: 37\tLoss: 0.16548027017784073\nEpoch: 38\tLoss: 0.17128210472007063\nEpoch: 39\tLoss: 0.15688980214416082\nEpoch: 40\tLoss: 0.1487816791108476\nEpoch: 41\tLoss: 0.1503905491266222\nEpoch: 42\tLoss: 0.14663880964242437\nEpoch: 43\tLoss: 0.15244457702685468\nEpoch: 44\tLoss: 0.14431282524324698\nEpoch: 45\tLoss: 0.1434988707371062\nEpoch: 46\tLoss: 0.1428590368869966\nEpoch: 47\tLoss: 0.14541045035651035\nEpoch: 48\tLoss: 0.14325361615157906\nEpoch: 49\tLoss: 0.14029049924033848\nEpoch: 50\tLoss: 0.13961209919386577\nEpoch: 51\tLoss: 0.140702241205792\nEpoch: 52\tLoss: 0.13845884047958998\nEpoch: 53\tLoss: 0.13744284513515265\nEpoch: 54\tLoss: 0.1489749758616638\nEpoch: 55\tLoss: 0.1372490941595899\nEpoch: 56\tLoss: 0.13604121297488336\nEpoch: 57\tLoss: 0.13608544133661685\nEpoch: 58\tLoss: 0.1375047195481557\nEpoch: 59\tLoss: 0.13844704210458\nEpoch: 60\tLoss: 0.13322503244759218\nEpoch: 61\tLoss: 0.13515767225102607\nEpoch: 62\tLoss: 0.1324793032043084\nEpoch: 63\tLoss: 0.13229882141708169\nEpoch: 64\tLoss: 0.13303394009453234\nEpoch: 65\tLoss: 0.13091386224479423\nEpoch: 66\tLoss: 0.13037412968025205\nEpoch: 67\tLoss: 0.13108402682306666\nEpoch: 68\tLoss: 0.12923735335997064\nEpoch: 69\tLoss: 0.12939165630622146\nEpoch: 70\tLoss: 0.1496733967017436\nEpoch: 71\tLoss: 0.12849736680804633\nEpoch: 72\tLoss: 0.1276434432639757\nEpoch: 73\tLoss: 0.13912411502061253\nEpoch: 74\tLoss: 0.13205042216800855\nEpoch: 75\tLoss: 0.1314303111379479\nEpoch: 76\tLoss: 0.1286692399379727\nEpoch: 77\tLoss: 0.1276172144543635\nEpoch: 78\tLoss: 0.1254510221563698\nEpoch: 79\tLoss: 0.1250279354256462\nEpoch: 80\tLoss: 0.12462017103913053\nEpoch: 81\tLoss: 0.12404859448756252\nEpoch: 82\tLoss: 0.13291927921589708\nEpoch: 83\tLoss: 0.12350198162949638\nEpoch: 84\tLoss: 0.12392687774771519\nEpoch: 85\tLoss: 0.12260131823047533\nEpoch: 86\tLoss: 0.12539080304217223\nEpoch: 87\tLoss: 0.13285428586426712\nEpoch: 88\tLoss: 0.1215092431905377\nEpoch: 89\tLoss: 0.12155597549297223\nEpoch: 90\tLoss: 0.12097440814260754\nEpoch: 91\tLoss: 0.12083818932565508\nEpoch: 92\tLoss: 0.12028112320586505\nEpoch: 93\tLoss: 0.12416961830429912\nEpoch: 94\tLoss: 0.12104737901925777\nEpoch: 95\tLoss: 0.11966212903427827\nEpoch: 96\tLoss: 0.11911782659414898\nEpoch: 97\tLoss: 0.11976758864780106\nEpoch: 98\tLoss: 0.11894235137177415\nEpoch: 99\tLoss: 0.12063852961612172\nEpoch: 100\tLoss: 0.12100517964054389\nAccuracy: 0.9736842105263158\n"
    }
   ],
   "source": [
    "logistic_regression_model = LogisticRegression(X_train, y_train)\n",
    "W, loss_hist = logistic_regression_model.fit(lr=1.5, nepoches=100, hist=True)\n",
    "logistic_precision = logistic_regression_model.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.956140350877193\n"
    }
   ],
   "source": [
    "svm_model = SVM(X_train, y_train)\n",
    "W, b = svm_model.fit()\n",
    "svm_precision = svm_model.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 1\tLoss: 0.5350317273833542\nEpoch: 2\tLoss: 0.41632238368078783\nEpoch: 3\tLoss: 0.3732847333770637\nEpoch: 4\tLoss: 0.33570541141733057\nEpoch: 5\tLoss: 0.33961871768834434\nEpoch: 6\tLoss: 0.30408055919851107\nEpoch: 7\tLoss: 0.2891699513664283\nEpoch: 8\tLoss: 0.2813872389786229\nEpoch: 9\tLoss: 0.272991187101826\nEpoch: 10\tLoss: 0.26935782127041064\nEpoch: 11\tLoss: 0.26214318881714377\nEpoch: 12\tLoss: 0.26202827939113904\nEpoch: 13\tLoss: 0.26811953743188155\nEpoch: 14\tLoss: 0.2507705085186109\nEpoch: 15\tLoss: 0.24279067430268325\nEpoch: 16\tLoss: 0.24402129058809957\nEpoch: 17\tLoss: 0.23901752136757407\nEpoch: 18\tLoss: 0.23487174077905934\nEpoch: 19\tLoss: 0.23511319762467897\nEpoch: 20\tLoss: 0.235637147108627\nEpoch: 21\tLoss: 0.23138062246440036\nEpoch: 22\tLoss: 0.22703003777524272\nEpoch: 23\tLoss: 0.22710125730287992\nEpoch: 24\tLoss: 0.22489404273494304\nEpoch: 25\tLoss: 0.22331848794734294\nEpoch: 26\tLoss: 0.22236761260058668\nEpoch: 27\tLoss: 0.22638753807467935\nEpoch: 28\tLoss: 0.22287771530342657\nEpoch: 29\tLoss: 0.2200783003905695\nEpoch: 30\tLoss: 0.21894537161370256\nEpoch: 31\tLoss: 0.2277854639951002\nEpoch: 32\tLoss: 0.21815806461444642\nEpoch: 33\tLoss: 0.2172985084284607\nEpoch: 34\tLoss: 0.22830071764518028\nEpoch: 35\tLoss: 0.21609642131293255\nEpoch: 36\tLoss: 0.21616268062150285\nEpoch: 37\tLoss: 0.21564408734465218\nEpoch: 38\tLoss: 0.21502573388337656\nEpoch: 39\tLoss: 0.21510683574078618\nEpoch: 40\tLoss: 0.21442813256142274\nEpoch: 41\tLoss: 0.2144032897832857\nEpoch: 42\tLoss: 0.21500960088745905\nEpoch: 43\tLoss: 0.21486663962702618\nEpoch: 44\tLoss: 0.22064198494929757\nEpoch: 45\tLoss: 0.21538610120193652\nEpoch: 46\tLoss: 0.21342930020058246\nEpoch: 47\tLoss: 0.2129686887333665\nEpoch: 48\tLoss: 0.21452590374491887\nEpoch: 49\tLoss: 0.2138149489860304\nEpoch: 50\tLoss: 0.2135650881173697\nEpoch: 51\tLoss: 0.21299353579906877\nEpoch: 52\tLoss: 0.21543835620429758\nEpoch: 53\tLoss: 0.21320883807103772\nEpoch: 54\tLoss: 0.21221117317622878\nEpoch: 55\tLoss: 0.2121389204538055\nEpoch: 56\tLoss: 0.21206383879344265\nEpoch: 57\tLoss: 0.21415666577386688\nEpoch: 58\tLoss: 0.21216354225545106\nEpoch: 59\tLoss: 0.21240765470887477\nEpoch: 60\tLoss: 0.21180601510250321\nEpoch: 61\tLoss: 0.2138319522275982\nEpoch: 62\tLoss: 0.21449284991540243\nEpoch: 63\tLoss: 0.21504651173497094\nEpoch: 64\tLoss: 0.213979796915531\nEpoch: 65\tLoss: 0.2115785553456702\nEpoch: 66\tLoss: 0.21515923010878163\nEpoch: 67\tLoss: 0.21166422645306499\nEpoch: 68\tLoss: 0.2114966723887958\nEpoch: 69\tLoss: 0.21248570694406813\nEpoch: 70\tLoss: 0.21348726851112146\nEpoch: 71\tLoss: 0.21196868565852867\nEpoch: 72\tLoss: 0.21154463523997968\nEpoch: 73\tLoss: 0.21339639485170744\nEpoch: 74\tLoss: 0.21212594699344658\nEpoch: 75\tLoss: 0.21267458732249855\nEpoch: 76\tLoss: 0.21145740168367813\nEpoch: 77\tLoss: 0.2148315285534931\nEpoch: 78\tLoss: 0.2112550741691061\nEpoch: 79\tLoss: 0.21371576925379962\nEpoch: 80\tLoss: 0.21131344830933133\nEpoch: 81\tLoss: 0.21159816002258647\nEpoch: 82\tLoss: 0.21124821898777338\nEpoch: 83\tLoss: 0.21138898025291036\nEpoch: 84\tLoss: 0.219202711504977\nEpoch: 85\tLoss: 0.21132839173116322\nEpoch: 86\tLoss: 0.21149519896495858\nEpoch: 87\tLoss: 0.21125619739099283\nEpoch: 88\tLoss: 0.2125781989680779\nEpoch: 89\tLoss: 0.2154352273633603\nEpoch: 90\tLoss: 0.22075920405746835\nEpoch: 91\tLoss: 0.2112989601450616\nEpoch: 92\tLoss: 0.2115819028646673\nEpoch: 93\tLoss: 0.21129555241158948\nEpoch: 94\tLoss: 0.2113325999814276\nEpoch: 95\tLoss: 0.2113957079206536\nEpoch: 96\tLoss: 0.21120222280743708\nEpoch: 97\tLoss: 0.2113234921702831\nEpoch: 98\tLoss: 0.21157958976197688\nEpoch: 99\tLoss: 0.21229071922047507\nEpoch: 100\tLoss: 0.21117710523157804\nAccuracy: 0.9824561403508771\n"
    }
   ],
   "source": [
    "softmargin_svm_model = SoftMarginSVM(X_train, y_train, l=0.01)\n",
    "W, b, loss_hist = softmargin_svm_model.fit(lr=0.1, nepoches=100, hist=True)\n",
    "softmargin_svm_precision = softmargin_svm_model.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}